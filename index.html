<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Neural Ambisonics Encoding with 2D Circular Microphone Array Captures"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="https://bridgoon97.github.io/NeuralAmbisonicsEncoding/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Neural Ambisonics Encoding with 2D Circular Microphone Array Captures">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Neural Ambisonics Encoding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Neural Ambisonics Encoding with 2D Circular Microphone Array Captures</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Neural Ambisonics Encoding with 2D Circular Microphone Array Captures</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://bridgoon97.github.io/" target="_blank">Yue Qiao</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/vinaykothapally/" target="_blank">Vinay Kothapally</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://raymond-myu.github.io/" target="_blank">Meng Yu</a><sup>2</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/view/dongyu888/" target="_blank">Dong Yu</a><sup>2</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Princeton University, <sup>2</sup>Tencent AI Lab</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This work presents a deep neural network (DNN)-based method for Ambisonics encoding in multi-speaker environments using a circular microphone array. Traditional Ambisonics encoding methods impose strict requirements on microphone array layouts, and while existing DNN-based approaches offer greater flexibility, they often struggle to generalize to non-ideal arrays. The proposed method employs a two-stage architecture that mimics the sound field decomposition and synthesis process. A loss function based on spatial power maps is introduced to regularize Ambisonics channel correlations, along with a channel permutation technique to address the ambiguity in encoding vertical information with a horizontal array. Evaluation on simulated speech and noise datasets against existing encoding methods shows that the proposed method achieves comparable timbral audio quality, significantly improved spatial audio quality, and enhanced source localization accuracy, making it particularly suitable for speech-focused applications such as teleconferencing.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Body -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Network Architecture</h2>
        <p>
          The proposed neural network architecture consists of two stages: In the first stage, the complex ratio filters (CRFs) are estimated to generate <i>L</i> virtual loudspeaker signals, conceptually similar to the plane wave decomposition of the captured sound field; in the second stage, another set of CRFs are estimated to spatially transform the virtual loudspeaker signals to the Ambisonics domain, similar to the sound field synthesis process. 
        </p><br>
        <figure class="image">
          <img src="static/images/architecture_v2.jpg" alt="Network Architecture" style="width:70%; margin: 0 auto;">
        </figure>
        <br>
      <h2 class="title is-3">Binaural Audio Demos with Spatial Power Map Visualization </h2>
      <p>
        All binaural audio demos are decoded from the ground truth and estimated second-order Ambisonics with different encoding methods (see the paper for details), using the KEMAR HRTF dataset<sup>[1]</sup>. The spatial power map in the video is generated by processing the Ambisonics signals with the <a href="https://leomccormack.github.io/sparta-site/docs/plugins/sparta-suite/#powermap">SPARTA PowerMap plugin</a><sup>[2]</sup> under the "PWD" mode. The power map is visualized as a heatmap, where the color represents the sound intensity at different directions. The azimuth angles are defined in the range of [-180°, 180°], where 0°, 90°, and -90° are the front, left, and right directions, respectively. The elevation angles are defined in the range of [-90°, 90°], where 90°, 0°, and -90° are the top, horizontal, and bottom directions, respectively. Headphones are recommended for the intended spatial audio experience.
      </p>
      <br>
      <h3 class="title is-4">Single-speaker scenarios (3 samples)</h3>
      <table class="table is-fullwidth custom-table" style="text-align:center">
        <thead>
          <tr>
            <th></th>
            <th>Reference</th>
            <th>LS-based Filtering II</th>
            <th>U-net-based<sup>[3]</sup></th>
            <th>Proposed</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Sample 1</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_32_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_32_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_32_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_32_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
          <tr>
            <td>Sample 2</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_73_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_73_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_73_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_73_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
          <tr>
            <td>Sample 3</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_428_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_428_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_428_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_428_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody>
      </table>

      <h3 class="title is-4">Two-speaker scenarios (3 samples)</h3>
      <table class="table is-fullwidth custom-table" style="text-align:center">
        <thead>
          <tr>
            <th></th>
            <th>Reference</th>
            <th>LS-based Filtering II</th>
            <th>U-net-based<sup>[3]</sup></th>
            <th>Proposed</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Sample 4</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_35_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_35_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_35_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_35_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
          <tr>
            <td>Sample 5</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_139_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_139_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_139_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_139_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
          <tr>
            <td>Sample 6</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_242_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_242_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_242_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_242_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody>
      </table>

      <h3 class="title is-4">Three-speaker scenarios (3 samples)</h3>
      <table class="table is-fullwidth custom-table" style="text-align:center">
        <thead>
          <tr>
            <th></th>
            <th>Reference</th>
            <th>LS-based Filtering II</th>
            <th>U-net-based<sup>[3]</sup></th>
            <th>Proposed</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Sample 7</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_22_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_22_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_22_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_22_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
          <tr>
            <td>Sample 8</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_58_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_58_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_58_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_58_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
          <tr>
            <td>Sample 9</td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_gt/eval_310_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_LS/eval_310_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_Unet/eval_310_UP.mov" type="video/mp4">
              </video>
            </td>
            <td>
              <video controls width="100%">
                <source src="static/videos/binaural_proposed/eval_310_UP.mov" type="video/mp4">
              </video>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- References -->
<section class="section" id="References">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>
    <p>
      [1] Gardner, William G., and Keith D. Martin. "HRTF measurements of a KEMAR." The Journal of the Acoustical Society of America 97.6 (1995): 3907-3908.
    </p>
    <p>
      [2] McCormack, Leo, and Archontis Politis. "SPARTA & COMPASS: Real-time implementations of linear and parametric spatial audio reproduction and processing methods." AES International Conference on Immersive and Interactive Audio. Audio Engineering Society, 2019.
    </p>
    <p>
      [3] Heikkinen, Mikko, Archontis Politis, and Tuomas Virtanen. "Neural Ambisonics encoding for compact irregular microphone arrays." ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024.
    </p>
  </div>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
  
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            </p>
  
          </div>
        </div>
      </div>
    </div>
  </footer>
<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
